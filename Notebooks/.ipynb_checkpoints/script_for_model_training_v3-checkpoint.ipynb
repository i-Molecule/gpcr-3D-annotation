{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_dir = \"/home/ilya/Box/GPCRapa/github/GPCRapa/\"\n",
    "\n",
    "\n",
    "dir_path = os.path.join(working_dir,\"test\")# path to working directory\n",
    "out_path = os.path.join(working_dir,\"test\")# path to output directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_features = os.path.join(working_dir,\"Files/resources/calculated_features1_03_21.csv\")\n",
    "path_to_state_mapping = os.path.join(working_dir,\"Files/resources/data_GPCR.csv\")\n",
    "path_to_mod_desc = os.path.join(working_dir,\"Files/resources/models_description.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdb_list = [\n",
    "    '6C1R','4NTJ','5NJ6','5VBL','4DKL','4DJH','2YDO','5UIW','6KO5','5ZKQ',\n",
    "    '5G53','5XSZ','5XJM','5GLH','5UNH','2YDV','5UEN','3V2Y','4UHR','5T1A',\n",
    "    '6DO1','6IIV','4EIY','4ZUD','5WB1','6ME2','4XT1','5XPR','6LFL','5ZBQ',\n",
    "    '6GDG','4PXZ','6M9T','6D26','6LFO','5DHH','4XNW','6B73','3VW7','4XT3',\n",
    "    '5TZY','6D9H','6DDE','4EJ4','6ME8','4Z36','4RWD']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = pd.read_csv(path_to_features)\n",
    "mod_descr = pd.read_csv(path_to_mod_desc)\n",
    "state_mapping = pd.read_csv(path_to_state_mapping)\n",
    "\n",
    "\n",
    "#make a dict with conformational state annotation for the dataset structures\n",
    "state_mapping_cut = state_mapping[state_mapping[\"PDB code\"].isin(pdb_list)][[\"PDB code\", \"State\"]]\n",
    "state_mapping_dict = {}\n",
    "for item1, item2 in zip(state_mapping_cut[\"PDB code\"].tolist(), state_mapping_cut[\"State\"].tolist()):\n",
    "    state_mapping_dict[item1] = item2\n",
    "active_structures = state_mapping_cut[state_mapping_cut[\"State\"] == \"active\"]\n",
    "active_structures = active_structures[\"PDB code\"].tolist()\n",
    "inactive_structures = state_mapping_cut[state_mapping_cut[\"State\"] == \"inactive\"]\n",
    "inactive_structures = inactive_structures[\"PDB code\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Model</th>\n",
       "      <th>Training_set_actve</th>\n",
       "      <th>Training_set_full</th>\n",
       "      <th>Test_set_active</th>\n",
       "      <th>Test_set_full</th>\n",
       "      <th>pres</th>\n",
       "      <th>rec</th>\n",
       "      <th>acc</th>\n",
       "      <th>balans_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>model_0</td>\n",
       "      <td>['4XT3', '5G53', '6B73', '6D9H', '6DDE', '6DO1...</td>\n",
       "      <td>['4XT3', '5G53', '6B73', '6D9H', '6DDE', '6DO1...</td>\n",
       "      <td>['4XT1', '5XJM']</td>\n",
       "      <td>['4XT1', '5XJM', '3V2Y', '4DKL', '4XNW', '5T1A...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>model_1</td>\n",
       "      <td>['4XT3', '5G53', '5XJM', '6B73', '6DDE', '6DO1...</td>\n",
       "      <td>['4XT3', '5G53', '5XJM', '6B73', '6DDE', '6DO1...</td>\n",
       "      <td>['4XT1', '6D9H']</td>\n",
       "      <td>['4XT1', '6D9H', '4EJ4', '5DHH', '5VBL', '5XPR...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>model_2</td>\n",
       "      <td>['4XT3', '5G53', '5XJM', '6B73', '6D9H', '6DO1...</td>\n",
       "      <td>['4XT3', '5G53', '5XJM', '6B73', '6D9H', '6DO1...</td>\n",
       "      <td>['4XT1', '6DDE']</td>\n",
       "      <td>['4XT1', '6DDE', '4EJ4', '5NJ6', '5T1A', '5XPR...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>model_3</td>\n",
       "      <td>['4XT1', '5XJM', '6B73', '6D9H', '6DDE', '6DO1...</td>\n",
       "      <td>['4XT1', '5XJM', '6B73', '6D9H', '6DDE', '6DO1...</td>\n",
       "      <td>['4XT3', '5G53']</td>\n",
       "      <td>['4XT3', '5G53', '4EJ4', '4XNW', '4Z36', '5VBL...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>model_4</td>\n",
       "      <td>['4XT1', '5G53', '5XJM', '6B73', '6D9H', '6DDE...</td>\n",
       "      <td>['4XT1', '5G53', '5XJM', '6B73', '6D9H', '6DDE...</td>\n",
       "      <td>['4XT3', '6DO1']</td>\n",
       "      <td>['4XT3', '6DO1', '4XNW', '5XPR', '5ZBQ', '6D26...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>model_5</td>\n",
       "      <td>['4XT1', '5G53', '5XJM', '6B73', '6D9H', '6DDE...</td>\n",
       "      <td>['4XT1', '5G53', '5XJM', '6B73', '6D9H', '6DDE...</td>\n",
       "      <td>['4XT3', '6LFO']</td>\n",
       "      <td>['4XT3', '6LFO', '4DKL', '5NJ6', '5ZBQ', '6D26...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>model_6</td>\n",
       "      <td>['4XT1', '4XT3', '5XJM', '6B73', '6DDE', '6DO1...</td>\n",
       "      <td>['4XT1', '4XT3', '5XJM', '6B73', '6DDE', '6DO1...</td>\n",
       "      <td>['5G53', '6D9H']</td>\n",
       "      <td>['5G53', '6D9H', '4DJH', '4EJ4', '5XPR', '5ZKQ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>model_7</td>\n",
       "      <td>['4XT1', '4XT3', '5G53', '6B73', '6DDE', '6DO1...</td>\n",
       "      <td>['4XT1', '4XT3', '5G53', '6B73', '6DDE', '6DO1...</td>\n",
       "      <td>['5XJM', '6D9H']</td>\n",
       "      <td>['5XJM', '6D9H', '4DKL', '4XNW', '4ZUD', '5NJ6...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>model_8</td>\n",
       "      <td>['4XT1', '4XT3', '5G53', '6B73', '6D9H', '6DDE...</td>\n",
       "      <td>['4XT1', '4XT3', '5G53', '6B73', '6D9H', '6DDE...</td>\n",
       "      <td>['5XJM', '6LFO']</td>\n",
       "      <td>['5XJM', '6LFO', '4EJ4', '4NTJ', '4XNW', '5VBL...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>model_9</td>\n",
       "      <td>['4XT1', '4XT3', '5G53', '5XJM', '6B73', '6DDE...</td>\n",
       "      <td>['4XT1', '4XT3', '5G53', '5XJM', '6B73', '6DDE...</td>\n",
       "      <td>['6D9H', '6DO1']</td>\n",
       "      <td>['6D9H', '6DO1', '3VW7', '4DKL', '5NJ6', '5XPR...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>model_10</td>\n",
       "      <td>['4XT3', '5G53', '5XJM', '6B73', '6D9H', '6DO1...</td>\n",
       "      <td>['4XT3', '5G53', '5XJM', '6B73', '6D9H', '6DO1...</td>\n",
       "      <td>['6DDE', '4XT1']</td>\n",
       "      <td>['6DDE', '4XT1', '3VW7', '4EJ4', '4XNW', '5DHH...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>model_11</td>\n",
       "      <td>['4XT1', '5G53', '5XJM', '6B73', '6D9H', '6DO1...</td>\n",
       "      <td>['4XT1', '5G53', '5XJM', '6B73', '6D9H', '6DO1...</td>\n",
       "      <td>['6DDE', '4XT3']</td>\n",
       "      <td>['6DDE', '4XT3', '4RWD', '4XNW', '5NJ6', '5T1A...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>model_12</td>\n",
       "      <td>['4XT1', '4XT3', '5XJM', '6B73', '6D9H', '6DO1...</td>\n",
       "      <td>['4XT1', '4XT3', '5XJM', '6B73', '6D9H', '6DO1...</td>\n",
       "      <td>['6DDE', '5G53']</td>\n",
       "      <td>['6DDE', '5G53', '4Z36', '5UIW', '5VBL', '6C1R...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>model_13</td>\n",
       "      <td>['4XT1', '4XT3', '5G53', '5XJM', '6B73', '6DO1...</td>\n",
       "      <td>['4XT1', '4XT3', '5G53', '5XJM', '6B73', '6DO1...</td>\n",
       "      <td>['6DDE', '6D9H']</td>\n",
       "      <td>['6DDE', '6D9H', '4DKL', '4EJ4', '5ZBQ', '6C1R...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>model_14</td>\n",
       "      <td>['4XT1', '4XT3', '5G53', '5XJM', '6B73', '6D9H...</td>\n",
       "      <td>['4XT1', '4XT3', '5G53', '5XJM', '6B73', '6D9H...</td>\n",
       "      <td>['6DDE', '6LFO']</td>\n",
       "      <td>['6DDE', '6LFO', '3V2Y', '4DKL', '4EJ4', '4XNW...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>model_15</td>\n",
       "      <td>['4XT1', '5G53', '5XJM', '6B73', '6D9H', '6DDE...</td>\n",
       "      <td>['4XT1', '5G53', '5XJM', '6B73', '6D9H', '6DDE...</td>\n",
       "      <td>['6DO1', '4XT3']</td>\n",
       "      <td>['6DO1', '4XT3', '4DKL', '4EJ4', '5ZKQ', '6C1R...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>model_16</td>\n",
       "      <td>['4XT1', '4XT3', '5G53', '6B73', '6D9H', '6DDE...</td>\n",
       "      <td>['4XT1', '4XT3', '5G53', '6B73', '6D9H', '6DDE...</td>\n",
       "      <td>['6DO1', '5XJM']</td>\n",
       "      <td>['6DO1', '5XJM', '4DKL', '4XNW', '5DHH', '5NJ6...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>model_17</td>\n",
       "      <td>['4XT3', '5G53', '5XJM', '6B73', '6D9H', '6DDE...</td>\n",
       "      <td>['4XT3', '5G53', '5XJM', '6B73', '6D9H', '6DDE...</td>\n",
       "      <td>['6LFO', '4XT1']</td>\n",
       "      <td>['6LFO', '4XT1', '4XNW', '4ZUD', '5UEN', '5UIW...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>model_18</td>\n",
       "      <td>['4XT1', '4XT3', '5G53', '5XJM', '6B73', '6DDE...</td>\n",
       "      <td>['4XT1', '4XT3', '5G53', '5XJM', '6B73', '6DDE...</td>\n",
       "      <td>['6LFO', '6D9H']</td>\n",
       "      <td>['6LFO', '6D9H', '4NTJ', '4ZUD', '5T1A', '5VBL...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>model_19</td>\n",
       "      <td>['4XT1', '4XT3', '5G53', '5XJM', '6B73', '6D9H...</td>\n",
       "      <td>['4XT1', '4XT3', '5G53', '5XJM', '6B73', '6D9H...</td>\n",
       "      <td>['6LFO', '6DDE']</td>\n",
       "      <td>['6LFO', '6DDE', '3VW7', '4DKL', '4XNW', '5NJ6...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0     Model                                 Training_set_actve  \\\n",
       "0            0   model_0  ['4XT3', '5G53', '6B73', '6D9H', '6DDE', '6DO1...   \n",
       "1            1   model_1  ['4XT3', '5G53', '5XJM', '6B73', '6DDE', '6DO1...   \n",
       "2            2   model_2  ['4XT3', '5G53', '5XJM', '6B73', '6D9H', '6DO1...   \n",
       "3            3   model_3  ['4XT1', '5XJM', '6B73', '6D9H', '6DDE', '6DO1...   \n",
       "4            4   model_4  ['4XT1', '5G53', '5XJM', '6B73', '6D9H', '6DDE...   \n",
       "5            5   model_5  ['4XT1', '5G53', '5XJM', '6B73', '6D9H', '6DDE...   \n",
       "6            6   model_6  ['4XT1', '4XT3', '5XJM', '6B73', '6DDE', '6DO1...   \n",
       "7            7   model_7  ['4XT1', '4XT3', '5G53', '6B73', '6DDE', '6DO1...   \n",
       "8            8   model_8  ['4XT1', '4XT3', '5G53', '6B73', '6D9H', '6DDE...   \n",
       "9            9   model_9  ['4XT1', '4XT3', '5G53', '5XJM', '6B73', '6DDE...   \n",
       "10          10  model_10  ['4XT3', '5G53', '5XJM', '6B73', '6D9H', '6DO1...   \n",
       "11          11  model_11  ['4XT1', '5G53', '5XJM', '6B73', '6D9H', '6DO1...   \n",
       "12          12  model_12  ['4XT1', '4XT3', '5XJM', '6B73', '6D9H', '6DO1...   \n",
       "13          13  model_13  ['4XT1', '4XT3', '5G53', '5XJM', '6B73', '6DO1...   \n",
       "14          14  model_14  ['4XT1', '4XT3', '5G53', '5XJM', '6B73', '6D9H...   \n",
       "15          15  model_15  ['4XT1', '5G53', '5XJM', '6B73', '6D9H', '6DDE...   \n",
       "16          16  model_16  ['4XT1', '4XT3', '5G53', '6B73', '6D9H', '6DDE...   \n",
       "17          17  model_17  ['4XT3', '5G53', '5XJM', '6B73', '6D9H', '6DDE...   \n",
       "18          18  model_18  ['4XT1', '4XT3', '5G53', '5XJM', '6B73', '6DDE...   \n",
       "19          19  model_19  ['4XT1', '4XT3', '5G53', '5XJM', '6B73', '6D9H...   \n",
       "\n",
       "                                    Training_set_full   Test_set_active  \\\n",
       "0   ['4XT3', '5G53', '6B73', '6D9H', '6DDE', '6DO1...  ['4XT1', '5XJM']   \n",
       "1   ['4XT3', '5G53', '5XJM', '6B73', '6DDE', '6DO1...  ['4XT1', '6D9H']   \n",
       "2   ['4XT3', '5G53', '5XJM', '6B73', '6D9H', '6DO1...  ['4XT1', '6DDE']   \n",
       "3   ['4XT1', '5XJM', '6B73', '6D9H', '6DDE', '6DO1...  ['4XT3', '5G53']   \n",
       "4   ['4XT1', '5G53', '5XJM', '6B73', '6D9H', '6DDE...  ['4XT3', '6DO1']   \n",
       "5   ['4XT1', '5G53', '5XJM', '6B73', '6D9H', '6DDE...  ['4XT3', '6LFO']   \n",
       "6   ['4XT1', '4XT3', '5XJM', '6B73', '6DDE', '6DO1...  ['5G53', '6D9H']   \n",
       "7   ['4XT1', '4XT3', '5G53', '6B73', '6DDE', '6DO1...  ['5XJM', '6D9H']   \n",
       "8   ['4XT1', '4XT3', '5G53', '6B73', '6D9H', '6DDE...  ['5XJM', '6LFO']   \n",
       "9   ['4XT1', '4XT3', '5G53', '5XJM', '6B73', '6DDE...  ['6D9H', '6DO1']   \n",
       "10  ['4XT3', '5G53', '5XJM', '6B73', '6D9H', '6DO1...  ['6DDE', '4XT1']   \n",
       "11  ['4XT1', '5G53', '5XJM', '6B73', '6D9H', '6DO1...  ['6DDE', '4XT3']   \n",
       "12  ['4XT1', '4XT3', '5XJM', '6B73', '6D9H', '6DO1...  ['6DDE', '5G53']   \n",
       "13  ['4XT1', '4XT3', '5G53', '5XJM', '6B73', '6DO1...  ['6DDE', '6D9H']   \n",
       "14  ['4XT1', '4XT3', '5G53', '5XJM', '6B73', '6D9H...  ['6DDE', '6LFO']   \n",
       "15  ['4XT1', '5G53', '5XJM', '6B73', '6D9H', '6DDE...  ['6DO1', '4XT3']   \n",
       "16  ['4XT1', '4XT3', '5G53', '6B73', '6D9H', '6DDE...  ['6DO1', '5XJM']   \n",
       "17  ['4XT3', '5G53', '5XJM', '6B73', '6D9H', '6DDE...  ['6LFO', '4XT1']   \n",
       "18  ['4XT1', '4XT3', '5G53', '5XJM', '6B73', '6DDE...  ['6LFO', '6D9H']   \n",
       "19  ['4XT1', '4XT3', '5G53', '5XJM', '6B73', '6D9H...  ['6LFO', '6DDE']   \n",
       "\n",
       "                                        Test_set_full  pres  rec  acc  \\\n",
       "0   ['4XT1', '5XJM', '3V2Y', '4DKL', '4XNW', '5T1A...   1.0  1.0  1.0   \n",
       "1   ['4XT1', '6D9H', '4EJ4', '5DHH', '5VBL', '5XPR...   1.0  1.0  1.0   \n",
       "2   ['4XT1', '6DDE', '4EJ4', '5NJ6', '5T1A', '5XPR...   1.0  1.0  1.0   \n",
       "3   ['4XT3', '5G53', '4EJ4', '4XNW', '4Z36', '5VBL...   1.0  1.0  1.0   \n",
       "4   ['4XT3', '6DO1', '4XNW', '5XPR', '5ZBQ', '6D26...   1.0  1.0  1.0   \n",
       "5   ['4XT3', '6LFO', '4DKL', '5NJ6', '5ZBQ', '6D26...   1.0  1.0  1.0   \n",
       "6   ['5G53', '6D9H', '4DJH', '4EJ4', '5XPR', '5ZKQ...   1.0  1.0  1.0   \n",
       "7   ['5XJM', '6D9H', '4DKL', '4XNW', '4ZUD', '5NJ6...   1.0  1.0  1.0   \n",
       "8   ['5XJM', '6LFO', '4EJ4', '4NTJ', '4XNW', '5VBL...   1.0  1.0  1.0   \n",
       "9   ['6D9H', '6DO1', '3VW7', '4DKL', '5NJ6', '5XPR...   1.0  1.0  1.0   \n",
       "10  ['6DDE', '4XT1', '3VW7', '4EJ4', '4XNW', '5DHH...   1.0  1.0  1.0   \n",
       "11  ['6DDE', '4XT3', '4RWD', '4XNW', '5NJ6', '5T1A...   1.0  1.0  1.0   \n",
       "12  ['6DDE', '5G53', '4Z36', '5UIW', '5VBL', '6C1R...   1.0  1.0  1.0   \n",
       "13  ['6DDE', '6D9H', '4DKL', '4EJ4', '5ZBQ', '6C1R...   1.0  1.0  1.0   \n",
       "14  ['6DDE', '6LFO', '3V2Y', '4DKL', '4EJ4', '4XNW...   1.0  1.0  1.0   \n",
       "15  ['6DO1', '4XT3', '4DKL', '4EJ4', '5ZKQ', '6C1R...   1.0  1.0  1.0   \n",
       "16  ['6DO1', '5XJM', '4DKL', '4XNW', '5DHH', '5NJ6...   1.0  1.0  1.0   \n",
       "17  ['6LFO', '4XT1', '4XNW', '4ZUD', '5UEN', '5UIW...   1.0  1.0  1.0   \n",
       "18  ['6LFO', '6D9H', '4NTJ', '4ZUD', '5T1A', '5VBL...   1.0  1.0  1.0   \n",
       "19  ['6LFO', '6DDE', '3VW7', '4DKL', '4XNW', '5NJ6...   1.0  1.0  1.0   \n",
       "\n",
       "    balans_acc  \n",
       "0          1.0  \n",
       "1          1.0  \n",
       "2          1.0  \n",
       "3          1.0  \n",
       "4          1.0  \n",
       "5          1.0  \n",
       "6          1.0  \n",
       "7          1.0  \n",
       "8          1.0  \n",
       "9          1.0  \n",
       "10         1.0  \n",
       "11         1.0  \n",
       "12         1.0  \n",
       "13         1.0  \n",
       "14         1.0  \n",
       "15         1.0  \n",
       "16         1.0  \n",
       "17         1.0  \n",
       "18         1.0  \n",
       "19         1.0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod_descr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in case of reproducing alredy trained RF models\n",
    "train_active_set = [f.rstrip(\"'\").lstrip(\"'\") for f in mod_descr[mod_descr[\"Model\"] == \"model_19\"].iloc[0, 2].rstrip(']\"').lstrip('\"[').split(\", \")]\n",
    "train_inactive_set = [f for f in [f.rstrip(\"'\").lstrip(\"'\") for f in mod_descr[mod_descr[\"Model\"] == \"model_19\"].iloc[0, 3].rstrip(']\"').lstrip('\"[').split(\", \")] if f not in train_active_set]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_split(data:pd.DataFrame(),\n",
    "                 active_pdb_list:list,\n",
    "                 inactive_pdb_list:list,\n",
    "                 path_to_state_mapping:str,\n",
    "                 a_train = 8, ina_train =22) -> np.array:\n",
    "    \n",
    "    \"\"\"Creates custom split for the GPCR class A MD trajectories dataset, with the respect\n",
    "    of the dataset initial proportion. All frames from the trajectory go either in training\n",
    "    or test set. \n",
    "\n",
    "    Args:\n",
    "    \n",
    "      data:\n",
    "          Dataframe with calculated features of each MD frame of the dataset.\n",
    "          \n",
    "      active_pdb_list:\n",
    "          List which contains pdb ids that represent active structures from the dataset.\n",
    "          \n",
    "      inactive_pdb_list:\n",
    "          List which contains pdb ids that represent inactive structures from the dataset.\n",
    "          \n",
    "      path_to_state_mapping:\n",
    "          Path to the Dataframe that contains mapping for all the dataset structures,\n",
    "          particularly their state classification.\n",
    "          \n",
    "      a_train:\n",
    "          Number of active structures in training set (Default - 8).\n",
    "          \n",
    "      ina_train:\n",
    "          Number of active structures in training set (Default - 22).\n",
    "\n",
    "    Returns:\n",
    "    \n",
    "      Custom split of the dataset to training and test set with labelling. \n",
    "    \"\"\"\n",
    "    \n",
    "    import random\n",
    "    #split\n",
    "    a_pdb_train = random.sample(active_pdb_list, a_train)\n",
    "    a_pdb_test = [f for f in active_pdb_list if f not in a_pdb_train]\n",
    "    ina_pdb_train = random.sample(inactive_pdb_list, ina_train)\n",
    "    ina_pdb_test = [f for f in inactive_pdb_list if f not in ina_pdb_train]\n",
    "    \n",
    "    \n",
    "    #making of the mapping dict\n",
    "    state_mapping = pd.read_csv(path_to_state_mapping)\n",
    "    state_mapping_cut = state_mapping[state_mapping[\"PDB code\"].isin(pdb_list)][[\"PDB code\", \"State\"]]\n",
    "    state_mapping_dict = {}\n",
    "    for item1, item2 in zip(state_mapping_cut[\"PDB code\"].tolist(), state_mapping_cut[\"State\"].tolist()):\n",
    "        state_mapping_dict[item1] = item2\n",
    "    \n",
    "    \n",
    "    #data processing \n",
    "    data[\"Pdb_names\"] = [f.split(\"_\")[0] for f in feat[\"Unnamed: 0\"]]\n",
    "    cl = [f for f in feat.columns.tolist() if f not in [\"Unnamed: 0\", \"Pdb_names\"]]\n",
    "    \n",
    "    \n",
    "    #training set preparation\n",
    "    train_df = data[data[\"Pdb_names\"].isin(a_pdb_train+ina_pdb_train)].copy()\n",
    "    st =[]\n",
    "    for item in train_df[\"Unnamed: 0\"].tolist():\n",
    "        if state_mapping_dict[item.split(\"_\")[0]] == \"active\":\n",
    "            st.append(1)\n",
    "        elif state_mapping_dict[item.split(\"_\")[0]] == \"inactive\":\n",
    "            st.append(0)\n",
    "    train_labels = np.array(st)\n",
    "    train_array = train_df[cl].to_numpy()\n",
    "    \n",
    "    \n",
    "    #test set preparation\n",
    "    test_df = data[data[\"Pdb_names\"].isin(a_pdb_test+ina_pdb_test)].copy()\n",
    "    print(\"Test_set_structures:\", a_pdb_test+ina_pdb_test)\n",
    "    sf =[]\n",
    "    for item in test_df[\"Unnamed: 0\"].tolist():\n",
    "        if state_mapping_dict[item.split(\"_\")[0]] == \"active\":\n",
    "            sf.append(1)\n",
    "        elif state_mapping_dict[item.split(\"_\")[0]] == \"inactive\":\n",
    "            sf.append(0)\n",
    "    test_labels = np.array(sf)\n",
    "    test_array = test_df[cl].to_numpy()\n",
    "    \n",
    "    \n",
    "    \n",
    "    return train_array, test_array, train_labels, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test_set_structures: ['5XJM', '6LFO', '3V2Y', '4DJH', '4RWD', '5ZBQ', '6C1R', '6ME2']\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = custom_split(feat, active_structures, inactive_structures, path_to_state_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_19 reproduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.53303334,  5.09480943,  8.90566163, ..., 11.75284715,\n",
       "         5.91643599,  8.04526827],\n",
       "       [ 5.06099783,  6.18682764,  9.19739207, ..., 11.11570481,\n",
       "         5.85888491,  7.42189561],\n",
       "       [ 5.44398728,  5.19628306,  8.98876807, ...,  9.96657102,\n",
       "         5.52738764,  5.87586497],\n",
       "       ...,\n",
       "       [ 4.4115219 ,  5.52447273,  7.5842271 , ...,  6.09866479,\n",
       "         6.59354574,  5.88375006],\n",
       "       [ 5.02061413,  5.58184523,  8.24997294, ...,  6.25573668,\n",
       "         6.42211093,  5.86233673],\n",
       "       [ 4.49965099,  5.45993332,  8.1587748 , ...,  6.13005579,\n",
       "         7.1526984 ,  6.98890419]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = feat[feat[\"Pdb_names\"].isin(train_active_set + train_inactive_set)].copy().drop(columns=[\"Unnamed: 0\", \"Pdb_names\"], axis = 1).to_numpy()\n",
    "y_train = [1 if state_mapping_dict[f.split(\"_\")[0]] == \"active\" else 0 for f in feat[feat[\"Pdb_names\"].isin(train_active_set + train_inactive_set)][\"Pdb_names\"].to_list()]\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced_accuracy 1.0\n",
      "Accuracy 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score\n",
    "print(\"Balanced_accuracy\", balanced_accuracy_score(y_test, clf.predict(X_test)))\n",
    "print(\"Accuracy\", clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1206\n",
      "           1       1.00      1.00      1.00       402\n",
      "\n",
      "    accuracy                           1.00      1608\n",
      "   macro avg       1.00      1.00      1.00      1608\n",
      "weighted avg       1.00      1.00      1.00      1608\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, clf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import svm\n",
    "clf = svm.SVC(kernel='rbf')\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1206\n",
      "           1       1.00      1.00      1.00       402\n",
      "\n",
      "    accuracy                           1.00      1608\n",
      "   macro avg       1.00      1.00      1.00      1608\n",
      "weighted avg       1.00      1.00      1.00      1608\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, clf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:17:30] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1206\n",
      "           1       1.00      1.00      1.00       402\n",
      "\n",
      "    accuracy                           1.00      1608\n",
      "   macro avg       1.00      1.00      1.00      1608\n",
      "weighted avg       1.00      1.00      1.00      1608\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ilya/anaconda3/envs/modelling/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "xg_clf = XGBClassifier(n_estimators=100)\n",
    "\n",
    "xg_clf.fit(X_train, y_train)\n",
    "print(classification_report(y_test, clf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random splits without repeating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_split_2_6_wo_repeats(data:pd.DataFrame(),\n",
    "                                path_to_state_mapping:str,\n",
    "                                active_pdb_list:list,\n",
    "                                inactive_pdb_list:list,\n",
    "                                a_train = 2,\n",
    "                                n_in_str_max = 6,\n",
    "                                n_in_str_min = 5,\n",
    "                                n_samples_26 = 3,\n",
    "                                n_samples_25 = 2,\n",
    "                                seed = 42) -> list:\n",
    "    \n",
    "    \n",
    "    \"\"\"Creates custom split for the GPCR class A MD trajectories dataset for 5-fold CV, with the respect\n",
    "    of the dataset initial proportion. All frames from the trajectory go either in training or test set. \n",
    "\n",
    "    Args:\n",
    "    \n",
    "      data:\n",
    "          Dataframe with calculated features of each MD frame of the dataset.\n",
    "     \n",
    "      path_to_state_mapping:\n",
    "      \n",
    "          Path to the Dataframe that contains mapping for all the dataset structures, particularly their\n",
    "          state classification.\n",
    "          \n",
    "      active_pdb_list:\n",
    "          List which contains pdb ids that represent active structures from the dataset.\n",
    "          \n",
    "      inactive_pdb_list:\n",
    "          List which contains pdb ids that represent inactive structures from the dataset.\n",
    "          \n",
    "      a_train:\n",
    "          Number of active structures in training set (Default - 8).\n",
    "          \n",
    "      n_in_str_max:\n",
    "          Maximum number of inactive strucutes in samples for CV (Default - 6).\n",
    "          \n",
    "      n_in_str_min:\n",
    "          Minimum number of inactive strucutes in samples for CV (Default - 5).\n",
    "          \n",
    "      n_samples_26:\n",
    "          Number of samples with 2 active and 6 inactive structures for CV (Default - 3).\n",
    "          \n",
    "      n_samples_25:\n",
    "          Number of of samples with 2 active and 5 inactive structures for CV (Default - 2).\n",
    "          \n",
    "      seed:\n",
    "          Random seed.\n",
    "          \n",
    "    Returns:\n",
    "    \n",
    "      List, containing the splits for 5-fold CV. Each custom split consists of training and\n",
    "      test set with labelling.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    import random\n",
    "    #разбивка\n",
    "    i=0\n",
    "    cv_sets_list = []\n",
    "    added_pdbs = []\n",
    "    random.seed(seed)\n",
    "    while i<n_samples_26:\n",
    "        \n",
    "        active_pdb_set = random.sample([f for f in active_pdb_list if f not in added_pdbs], a_train)\n",
    "        inactive_pdb_set = random.sample([f for f in inactive_pdb_list if f not in added_pdbs], n_in_str_max)\n",
    "        cv_sets_list.append(active_pdb_set+inactive_pdb_set)\n",
    "        added_pdbs+=active_pdb_set+inactive_pdb_set\n",
    "        i+=1\n",
    "    \n",
    "    i=0\n",
    "    while i<n_samples_25:\n",
    "        \n",
    "        active_pdb_set = random.sample([f for f in active_pdb_list if f not in added_pdbs], a_train)\n",
    "        inactive_pdb_set = random.sample([f for f in inactive_pdb_list if f not in added_pdbs], n_in_str_min)\n",
    "        cv_sets_list.append(active_pdb_set+inactive_pdb_set)\n",
    "        added_pdbs+=active_pdb_set+inactive_pdb_set\n",
    "        i+=1\n",
    "    \n",
    "    print(cv_sets_list)\n",
    "    print(len(list(set(added_pdbs))))\n",
    "    \n",
    "    #making of the mapping dict\n",
    "    state_mapping = pd.read_csv(path_to_state_mapping)\n",
    "    state_mapping_cut = state_mapping[state_mapping[\"PDB code\"].isin(pdb_list)][[\"PDB code\", \"State\"]]\n",
    "    state_mapping_dict = {}\n",
    "    for item1, item2 in zip(state_mapping_cut[\"PDB code\"].tolist(), state_mapping_cut[\"State\"].tolist()):\n",
    "        state_mapping_dict[item1] = item2\n",
    "    \n",
    "    #data processing \n",
    "    data[\"Pdb_names\"] = [f.split(\"_\")[0] for f in feat[\"Unnamed: 0\"]]\n",
    "    cl = [f for f in feat.columns.tolist() if f not in [\"Unnamed: 0\", \"Pdb_names\"]]\n",
    "    \n",
    "    #CV sets preparation\n",
    "    cv_sets=[]\n",
    "    for test_set in cv_sets_list:\n",
    "        train_sets=[f for f in cv_sets_list if f != test_set]\n",
    "        \n",
    "        \n",
    "        #data for k-1 train\n",
    "        train_dfs=[]\n",
    "        for train_set in train_sets:\n",
    "            train_df = data[data[\"Pdb_names\"].isin(train_set)].copy()\n",
    "            train_dfs.append(train_df)\n",
    "        train_df = pd.concat(train_dfs)\n",
    "        print(train_df.shape)\n",
    "        \n",
    "        \n",
    "        #labels for k-1 train\n",
    "        train_labels=[]\n",
    "        for item in train_df[\"Unnamed: 0\"].tolist():\n",
    "            if state_mapping_dict[item.split(\"_\")[0]] == \"active\":\n",
    "                train_labels.append(1)\n",
    "            elif state_mapping_dict[item.split(\"_\")[0]] == \"inactive\":\n",
    "                train_labels.append(0)\n",
    "        \n",
    "        train_labels = np.array(train_labels)\n",
    "        train_array = train_df[cl].to_numpy()\n",
    "        \n",
    "        \n",
    "        #data for test\n",
    "        test_df = data[data[\"Pdb_names\"].isin(test_set)].copy()\n",
    "        print(test_df.shape)\n",
    "\n",
    "        \n",
    "        #labels for test\n",
    "        test_labels=[]\n",
    "        for item in test_df[\"Unnamed: 0\"].tolist():\n",
    "            if state_mapping_dict[item.split(\"_\")[0]] == \"active\":\n",
    "                test_labels.append(1)\n",
    "            elif state_mapping_dict[item.split(\"_\")[0]] == \"inactive\":\n",
    "                test_labels.append(0)\n",
    "        \n",
    "        test_labels = np.array(test_labels)\n",
    "        test_array = test_df[cl].to_numpy()\n",
    "        \n",
    "        \n",
    "        cv_sets.append([[test_labels,test_array],[train_labels,train_array]])\n",
    "        \n",
    "        \n",
    "    return cv_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['4XT3', '4XT1', '6IIV', '4XNW', '4RWD', '6ME8', '4EIY', '4DKL'], ['5XJM', '6DDE', '5XPR', '3VW7', '3V2Y', '4DJH', '4ZUD', '5DHH'], ['6GDG', '6LFO', '4EJ4', '5XSZ', '5NJ6', '6C1R', '5ZKQ', '6LFL'], ['6DO1', '5G53', '6D26', '5UIW', '4NTJ', '5ZBQ', '4Z36'], ['6D9H', '6B73', '5VBL', '5UEN', '5T1A', '6ME2', '6KO5']]\n",
      "38\n",
      "(6030, 40)\n",
      "(1608, 40)\n",
      "(6030, 40)\n",
      "(1608, 40)\n",
      "(6030, 40)\n",
      "(1608, 40)\n",
      "(6231, 40)\n",
      "(1407, 40)\n",
      "(6231, 40)\n",
      "(1407, 40)\n"
     ]
    }
   ],
   "source": [
    "cv_data_wo_rep_26 = custom_split_2_6_wo_repeats(feat, path_to_state_mapping,\n",
    "                                                active_structures, inactive_structures,\n",
    "                                                seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9968905472636815, 0.9975124378109452, 0.75, 1.0, 0.992181947405828] 0.9473169864960909 0.09869091047210349\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "cv_results = []\n",
    "for set_ in cv_data_wo_rep_26:\n",
    "    \n",
    "    \n",
    "    clf = RandomForestClassifier(n_estimators=100)\n",
    "    \n",
    "    \n",
    "    test_set = set_[0]\n",
    "    test_labels = test_set[0]\n",
    "    test_array = test_set[1]\n",
    "    \n",
    "    \n",
    "    train_set = set_[1]\n",
    "    train_labels = train_set[0]\n",
    "    train_array = train_set[1]\n",
    "    \n",
    "    clf.fit(train_array, train_labels)\n",
    "    cv_results.append(clf.score(test_array, test_labels))\n",
    "    \n",
    "    \n",
    "print(cv_results, np.mean(cv_results), np.std(cv_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/ilya/work/Projects/Article/Case_studies/script_for_model_training/models/rf_42.joblib']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(clf, '/home/ilya/work/Projects/Article/Case_studies/script_for_model_training/models/rf_42.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9720149253731343, 0.8799751243781094, 0.8675373134328358, 0.976545842217484, 0.9950248756218906] 0.9382196162046907 0.05334092007623909\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import svm\n",
    "cv_results = []\n",
    "for set_ in cv_data_wo_rep_26:\n",
    "    \n",
    "    \n",
    "    clf_svm = svm.SVC(kernel='rbf', probability=True)\n",
    "    \n",
    "    \n",
    "    test_set = set_[0]\n",
    "    test_labels = test_set[0]\n",
    "    test_array = test_set[1]\n",
    "    \n",
    "    \n",
    "    train_set = set_[1]\n",
    "    train_labels = train_set[0]\n",
    "    train_array = train_set[1]\n",
    "    \n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    train_array = scaler.fit_transform(train_array)\n",
    "    test_array = scaler.transform(test_array)\n",
    "\n",
    "    \n",
    "    clf_svm.fit(train_array, train_labels)\n",
    "    cv_results.append(clf_svm.score(test_array, test_labels))\n",
    "    \n",
    "    \n",
    "print(cv_results, np.mean(cv_results), np.std(cv_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/ilya/work/Projects/Article/Case_studies/script_for_model_training/models/scaler_for_svm_42.joblib']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(clf_svm, '/home/ilya/work/Projects/Article/Case_studies/script_for_model_training/models/svm_42.joblib')\n",
    "joblib.dump(scaler, '/home/ilya/work/Projects/Article/Case_studies/script_for_model_training/models/scaler_for_svm_42.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:04:58] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:04:58] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ilya/anaconda3/envs/modelling/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/ilya/anaconda3/envs/modelling/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/ilya/anaconda3/envs/modelling/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/ilya/anaconda3/envs/modelling/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:04:59] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:04:59] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:04:59] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0.9950248756218906, 0.9825870646766169, 0.75, 0.9992892679459844, 0.9466950959488273] 0.9347192608386639 0.09419342396139899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ilya/anaconda3/envs/modelling/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "cv_results = []\n",
    "for set_ in cv_data_wo_rep_26:\n",
    "    \n",
    "    \n",
    "    xg_clf = XGBClassifier(n_estimators=100)\n",
    "    \n",
    "    \n",
    "    test_set = set_[0]\n",
    "    test_labels = test_set[0]\n",
    "    test_array = test_set[1]\n",
    "    \n",
    "    \n",
    "    train_set = set_[1]\n",
    "    train_labels = train_set[0]\n",
    "    train_array = train_set[1]\n",
    "    \n",
    "    xg_clf.fit(train_array, train_labels)\n",
    "    cv_results.append(xg_clf.score(test_array, test_labels))\n",
    "    \n",
    "    \n",
    "print(cv_results, np.mean(cv_results), np.std(cv_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/ilya/work/Projects/Article/Case_studies/script_for_model_training/models/xg_42.joblib']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(xg_clf, '/home/ilya/work/Projects/Article/Case_studies/script_for_model_training/models/xg_42.joblib') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
